{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adb01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seleniumNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\bank of america\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.13.0 outcome-1.1.0 selenium-4.1.2 trio-0.20.0 trio-websocket-0.9.2 wsproto-1.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcf1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96c2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e5b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver= webdriver.Chrome('chromedriver.exe')# upload driver on jupyter root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "709bb059",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Job Title   2. Company name    3. Location of Job    4. Experience required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c739d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"1017b06d-733b-43fa-b291-462d0189117e\")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding web element for search job bar using id\n",
    "\n",
    "search_job =driver.find_element_by_class_name(\"suggestor-input\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2474fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar:\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5c34d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"db7991f2-adc9-49ed-b667-9dd348b09025\")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding web element for search location bar using absolute path\n",
    "\n",
    "search_location =driver.find_element_by_xpath( '/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_location                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8166580",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_location.send_keys('Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803c5327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"6834996d-fe84-493b-a2c2-cbc6715c4458\")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLick search button to search\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc8e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24a403",
   "metadata": {},
   "source": [
    "Clicking filters by selection check boxes using absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339729e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"d47f7c3d-0261-4ea5-a60e-f38c2af5aca9\")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_check=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[1]/label/i \")\n",
    "salary_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee53b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_check.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7dd0ba",
   "metadata": {},
   "source": [
    "Now lets first create 4 empty lists. in these list the data will be stored while scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0a414",
   "metadata": {},
   "source": [
    "# EXTRACTING JOB TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae6d1112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"1305bcf6-878b-4e6f-b0b9-ea9b6b139e32\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"11669efaefe4c9ae89e339ed0ffed356\", element=\"14c983e0-b183-4a6b-9460-4b94c6f00b58\")>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lETS EXTRACT ALL WEB ELEMENTS HAVING JOB TITLES\n",
    "\n",
    "\n",
    "title_tags=  driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "len(title_tags)\n",
    "title_tags[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e97dd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles=[]\n",
    "\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "len(job_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84250601",
   "metadata": {},
   "source": [
    "# Extracting Company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ce8fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tags= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"] ')\n",
    "len(company_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89400948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GABA Consultancy services', 'AddRec Solutions Pvt.Ltd.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_names=[]\n",
    "\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "company_names[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bc8e5",
   "metadata": {},
   "source": [
    "# Extracting experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47d76196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets extract all web elements having experience using parent tag  --> child\n",
    "\n",
    "exp_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc421e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-0 Yrs', '2-4 Yrs']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience=[]\n",
    "\n",
    "for i in exp_tags:\n",
    "    experience.append(i.text)\n",
    "len(experience)    \n",
    "experience[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6901693",
   "metadata": {},
   "source": [
    "# Extracting location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c880f541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "location_tag= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\" ]')\n",
    "len(location_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f28346a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noida, New Delhi, Gurgaon/Gurugram', 'Gurgaon/Gurugram']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "location=[]\n",
    "for i in location_tag:\n",
    "    location.append(i.text)\n",
    "    \n",
    "len(location)\n",
    "location[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02c7c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19, 19, 19)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_titles),len(company_names),len(experience),len(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08790d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs= pd.DataFrame()\n",
    "jobs['Job Titles']= job_titles\n",
    "jobs['Company Names']= company_names\n",
    "jobs['Experience']= experience\n",
    "jobs['Location']= location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9ff97f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Titles</th>\n",
       "      <th>Company Names</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only Fresher / Python Data Scientist / Trainee...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>AddRec Solutions Pvt.Ltd.</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>DECISION POINT PRIVATE LIMITED</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Chennai\\n(WFH during Covid)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Immediate Joiners only</td>\n",
       "      <td>MLAI Digital</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst / Data Scientist / Business Analy...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>Noida, New Delhi, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CNH Industrial</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gauge Data Solutions Pvt Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Alp Consulting Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Chennai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Cloudstrats Technologies Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>New Delhi, Delhi / NCR, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Required! Data Scientist Executives</td>\n",
       "      <td>Web King Services</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Delhi / NCR\\n(WFH duri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist,Data Analyst</td>\n",
       "      <td>Maximus Human Resources Private Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Developers, Data Scientists, Web Designers, An...</td>\n",
       "      <td>Cogmac Technologies</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Data Analyst 1</td>\n",
       "      <td>Axa XL</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Silokhera Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Senior Marketing Data Analyst</td>\n",
       "      <td>Tide Software</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Quality Analyst / Data Engineer / Associate An...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Noida, New Delhi, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Analog Legalhub Technology Solutions Pvt Ltd</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "      <td>New Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lead Data Analyst</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Grofers</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Titles  \\\n",
       "0   Only Fresher / Python Data Scientist / Trainee...   \n",
       "1                                      Data Scientist   \n",
       "2                               Senior Data Scientist   \n",
       "3             Data Scientist - Immediate Joiners only   \n",
       "4   Data Analyst / Data Scientist / Business Analy...   \n",
       "5                                      Data Scientist   \n",
       "6                                      Data Scientist   \n",
       "7                                      Data Scientist   \n",
       "8                                      Data Scientist   \n",
       "9                 Required! Data Scientist Executives   \n",
       "10                        Data Scientist,Data Analyst   \n",
       "11  Developers, Data Scientists, Web Designers, An...   \n",
       "12                              Senior Data Analyst 1   \n",
       "13                      Senior Marketing Data Analyst   \n",
       "14  Quality Analyst / Data Engineer / Associate An...   \n",
       "15                                Senior Data Analyst   \n",
       "16                                       Data Analyst   \n",
       "17                                  Lead Data Analyst   \n",
       "18                                       Data Analyst   \n",
       "\n",
       "                                   Company Names Experience  \\\n",
       "0                      GABA Consultancy services    0-0 Yrs   \n",
       "1                      AddRec Solutions Pvt.Ltd.    2-4 Yrs   \n",
       "2                 DECISION POINT PRIVATE LIMITED    2-5 Yrs   \n",
       "3                                   MLAI Digital    3-6 Yrs   \n",
       "4                      GABA Consultancy services    0-0 Yrs   \n",
       "5                                 CNH Industrial    1-6 Yrs   \n",
       "6                   Gauge Data Solutions Pvt Ltd    2-7 Yrs   \n",
       "7                         Alp Consulting Limited    1-6 Yrs   \n",
       "8       Cloudstrats Technologies Private Limited    3-5 Yrs   \n",
       "9                              Web King Services    0-5 Yrs   \n",
       "10       Maximus Human Resources Private Limited    1-6 Yrs   \n",
       "11                           Cogmac Technologies    1-5 Yrs   \n",
       "12                                        Axa XL    0-5 Yrs   \n",
       "13                                 Tide Software    0-4 Yrs   \n",
       "14                     GABA Consultancy services    0-5 Yrs   \n",
       "15                                       Grofers    4-7 Yrs   \n",
       "16  Analog Legalhub Technology Solutions Pvt Ltd    0-4 Yrs   \n",
       "17                                       Grofers    5-7 Yrs   \n",
       "18                                       Grofers    1-2 Yrs   \n",
       "\n",
       "                                             Location  \n",
       "0                  Noida, New Delhi, Gurgaon/Gurugram  \n",
       "1                                    Gurgaon/Gurugram  \n",
       "2       Gurgaon/Gurugram, Chennai\\n(WFH during Covid)  \n",
       "3                    Bangalore/Bengaluru, Delhi / NCR  \n",
       "4                       Noida, New Delhi, Delhi / NCR  \n",
       "5                                    Gurgaon/Gurugram  \n",
       "6                                               Noida  \n",
       "7   Kolkata, Hyderabad/Secunderabad, Pune, Chennai...  \n",
       "8          New Delhi, Delhi / NCR, Mumbai (All Areas)  \n",
       "9   Hyderabad/Secunderabad, Delhi / NCR\\n(WFH duri...  \n",
       "10                           Mumbai, Gurgaon/Gurugram  \n",
       "11                                              Noida  \n",
       "12                                  Silokhera Gurgaon  \n",
       "13                                   Gurgaon/Gurugram  \n",
       "14                      Noida, New Delhi, Delhi / NCR  \n",
       "15                                   Gurgaon/Gurugram  \n",
       "16                                          New Delhi  \n",
       "17                                   Gurgaon/Gurugram  \n",
       "18              Gurgaon/Gurugram, Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b46ad",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0c8e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "481484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a246b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Search bar for Job , location and enter data\n",
    "search_job= driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div/input')\n",
    "search_location= driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_job.send_keys('Data Analyst')\n",
    "search_location.send_keys('Bangalore')\n",
    "\n",
    "\n",
    "# Click search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004a422",
   "metadata": {},
   "source": [
    "# Find Job Title,  Job-Location , Company Name, Experience Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "25791644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>software developer &amp; Testing / Business Analys...</td>\n",
       "      <td>SECRET TECHNOLOGIES INDIA VMS GROUP</td>\n",
       "      <td>Pune, Bangalore/Bengaluru(Shivaji Nagar), Mumb...</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst - IIM/ISB/MDI/FMS/SP Jain</td>\n",
       "      <td>K12 Techno Services Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Mobile Premier League</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Looking For Data Analyst</td>\n",
       "      <td>Trellance</td>\n",
       "      <td>Ahmedabad, Bangalore/Bengaluru</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst || Advance Excel || D Limit || Co...</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst (Python, SQL, Excel)</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Staff Business Data Analyst - FDP</td>\n",
       "      <td>Intuit Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Capillary Technologies</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "1   software developer & Testing / Business Analys...   \n",
       "2              Data Analyst - IIM/ISB/MDI/FMS/SP Jain   \n",
       "3                                 Senior Data Analyst   \n",
       "4                                 Senior Data Analyst   \n",
       "5                            Looking For Data Analyst   \n",
       "6   Data Analyst || Advance Excel || D Limit || Co...   \n",
       "7                   Data Analyst (Python, SQL, Excel)   \n",
       "8                   Staff Business Data Analyst - FDP   \n",
       "9                                 Senior Data Analyst   \n",
       "10                                       Data Analyst   \n",
       "\n",
       "                                       Company  \\\n",
       "1          SECRET TECHNOLOGIES INDIA VMS GROUP   \n",
       "2                  K12 Techno Services Pvt Ltd   \n",
       "3                        Mobile Premier League   \n",
       "4                                     Flipkart   \n",
       "5                                    Trellance   \n",
       "6   Inspiration Manpower Consultancy Pvt. Ltd.   \n",
       "7                                     Flipkart   \n",
       "8                                  Intuit Inc.   \n",
       "9                                     Flipkart   \n",
       "10                      Capillary Technologies   \n",
       "\n",
       "                                             Location Experience  \n",
       "1   Pune, Bangalore/Bengaluru(Shivaji Nagar), Mumb...    0-4 Yrs  \n",
       "2                                 Bangalore/Bengaluru    4-9 Yrs  \n",
       "3                                 Bangalore/Bengaluru    3-6 Yrs  \n",
       "4                                 Bangalore/Bengaluru    3-6 Yrs  \n",
       "5                      Ahmedabad, Bangalore/Bengaluru    0-2 Yrs  \n",
       "6                                 Bangalore/Bengaluru    2-4 Yrs  \n",
       "7                                 Bangalore/Bengaluru    5-7 Yrs  \n",
       "8                                 Bangalore/Bengaluru    3-7 Yrs  \n",
       "9                                 Bangalore/Bengaluru    1-3 Yrs  \n",
       "10                                Bangalore/Bengaluru    5-8 Yrs  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tags= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "location_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "company_tags= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "exp_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')\n",
    "\n",
    "title=[]\n",
    "location=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "for i in title_tags:\n",
    "    title.append(i.text)\n",
    "for i in location_tags:\n",
    "    location.append(i.text)\n",
    "for i in company_tags:\n",
    "    company.append(i.text)\n",
    "for i in exp_tags:\n",
    "    exp.append(i.text)\n",
    "    \n",
    "    \n",
    "DataAnalyst=pd.DataFrame({'Job Title':title[0:10],'Company':company[0:10],'Location':location[0:10],'Experience':exp[0:10]})\n",
    "DataAnalyst.index +=1\n",
    "\n",
    "DataAnalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adff5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df4e91c4",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f8bd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "78ae6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "134fff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Search bar for Job , location and enter data\n",
    "search_job= driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div/input')\n",
    "search_location= driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "search_job.send_keys('Data Scientist')\n",
    "search_location.send_keys('Bangalore')\n",
    "\n",
    "\n",
    "# Click search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bab74366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist / Analyst</td>\n",
       "      <td>open data fabric</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Opportunity For Freshers For AI/ML, ...</td>\n",
       "      <td>NTT Data</td>\n",
       "      <td>Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hewlett-Packard (HP)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>newscorp</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Swiggy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Virtusa Hiring DATA Scientist- PAN India</td>\n",
       "      <td>Virtusa</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr. Product Data Scientist</td>\n",
       "      <td>Uber</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist/Senior Data Scientist - Python</td>\n",
       "      <td>ApicalGo Consultancy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title               Company  \\\n",
       "1                            Data Scientist / Analyst      open data fabric   \n",
       "2     Forecasting Analyst/ Data Scientist (US Client)            Concentrix   \n",
       "3   Excellent Opportunity For Freshers For AI/ML, ...              NTT Data   \n",
       "4                                      Data Scientist                Amazon   \n",
       "5                                      Data Scientist  Hewlett-Packard (HP)   \n",
       "6                               Senior Data Scientist              newscorp   \n",
       "7          Data Scientist II- Merchandise & Discovery                Swiggy   \n",
       "8            Virtusa Hiring DATA Scientist- PAN India               Virtusa   \n",
       "9                          Sr. Product Data Scientist                  Uber   \n",
       "10      Data Scientist/Senior Data Scientist - Python  ApicalGo Consultancy   \n",
       "\n",
       "                                             Location  \n",
       "1                                 Bangalore/Bengaluru  \n",
       "2   Gurgaon/Gurugram, Bangalore/Bengaluru\\n(WFH du...  \n",
       "3   Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...  \n",
       "4   Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru  \n",
       "5                                 Bangalore/Bengaluru  \n",
       "6                                 Bangalore/Bengaluru  \n",
       "7                                 Bangalore/Bengaluru  \n",
       "8   Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram...  \n",
       "9                                 Bangalore/Bengaluru  \n",
       "10                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tags= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "location_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "company_tags= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "\n",
    "title=[]\n",
    "location=[]\n",
    "company=[]\n",
    "\n",
    "for i in title_tags:\n",
    "    title.append(i.text)\n",
    "for i in location_tags:\n",
    "    location.append(i.text)\n",
    "for i in company_tags:\n",
    "    company.append(i.text)\n",
    "\n",
    "    \n",
    "    \n",
    "DataAnalyst=pd.DataFrame({'Job Title':title[0:10],'Company':company[0:10],'Location':location[0:10]})\n",
    "DataAnalyst.index +=1\n",
    "\n",
    "DataAnalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7ad4c",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a8b6e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "312e491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3f3782a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Search bar for Job , location and enter data\n",
    "search_job= driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div/input')\n",
    "search_job.send_keys('Data Scientist')\n",
    "\n",
    "# Click search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71ff0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the location 'Delhi/NCR'\n",
    "location_check= driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/i')\n",
    "location_check.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f54d8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the salary range  3-6 Lakhs\n",
    "\n",
    "salary_check= driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i')\n",
    "salary_check.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "89f140cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Excellent Opportunity For Freshers For AI/ML, ...</td>\n",
       "      <td>NTT Data</td>\n",
       "      <td>Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Analyst/ Scientist- Fresher Position</td>\n",
       "      <td>Sejal Consulting Hub</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only Fresher / Python Data Scientist / Trainee...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Noida, New Delhi, Gurgaon/Gurugram</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Indihire HR Consultants Private Limited</td>\n",
       "      <td>Delhi / NCR\\n(WFH during Covid)</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Optum</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>MOTHERSONSUMI INFOTECH &amp; DESIGNS LIMITED</td>\n",
       "      <td>Noida</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>Noida, Pune, Mumbai (All Areas)</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Open Project | IT Consulting | Data Scientist</td>\n",
       "      <td>Virtuoso Staffing Solutions Pvt Ltd</td>\n",
       "      <td>Noida, New Delhi, Bangalore Rural, Bangalore/B...</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Urgent Requirement || Data Scientist || Noida</td>\n",
       "      <td>HCL</td>\n",
       "      <td>Noida, Delhi / NCR</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  \\\n",
       "1   Excellent Opportunity For Freshers For AI/ML, ...   \n",
       "2    Junior Data Analyst/ Scientist- Fresher Position   \n",
       "3   Only Fresher / Python Data Scientist / Trainee...   \n",
       "4                                 Lead Data Scientist   \n",
       "5                                      Data Scientist   \n",
       "6                      Data Scientist - MIND Infotech   \n",
       "7                      Data Scientist - MIND Infotech   \n",
       "8                           Hiring For Data Scientist   \n",
       "9       Open Project | IT Consulting | Data Scientist   \n",
       "10      Urgent Requirement || Data Scientist || Noida   \n",
       "\n",
       "                                     Company  \\\n",
       "1                                   NTT Data   \n",
       "2                       Sejal Consulting Hub   \n",
       "3                  GABA Consultancy services   \n",
       "4    Indihire HR Consultants Private Limited   \n",
       "5                                      Optum   \n",
       "6   MOTHERSONSUMI INFOTECH & DESIGNS LIMITED   \n",
       "7   MOTHERSONSUMI INFOTECH & DESIGNS LIMITED   \n",
       "8                                   Lumiq.ai   \n",
       "9        Virtuoso Staffing Solutions Pvt Ltd   \n",
       "10                                       HCL   \n",
       "\n",
       "                                             Location Experience  \n",
       "1   Noida, Kolkata, Hyderabad/Secunderabad, Pune, ...    0-0 Yrs  \n",
       "2   Kolkata, Hyderabad/Secunderabad, Pune, Ahmedab...    0-3 Yrs  \n",
       "3                  Noida, New Delhi, Gurgaon/Gurugram    0-0 Yrs  \n",
       "4                     Delhi / NCR\\n(WFH during Covid)    2-4 Yrs  \n",
       "5                             Noida, Gurgaon/Gurugram    2-6 Yrs  \n",
       "6                                               Noida    4-8 Yrs  \n",
       "7                                               Noida    4-8 Yrs  \n",
       "8                     Noida, Pune, Mumbai (All Areas)    2-7 Yrs  \n",
       "9   Noida, New Delhi, Bangalore Rural, Bangalore/B...   5-10 Yrs  \n",
       "10                                 Noida, Delhi / NCR    3-8 Yrs  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tags= driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "location_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "company_tags= driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "exp_tags= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')\n",
    "\n",
    "title=[]\n",
    "location=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "for i in title_tags:\n",
    "    title.append(i.text)\n",
    "for i in location_tags:\n",
    "    location.append(i.text)\n",
    "for i in company_tags:\n",
    "    company.append(i.text)\n",
    "for i in exp_tags:\n",
    "    exp.append(i.text)\n",
    "    \n",
    "    \n",
    "DataAnalyst=pd.DataFrame({'Job Title':title[0:10],'Company':company[0:10],'Location':location[0:10],'Experience':exp[0:10]})\n",
    "DataAnalyst.index +=1\n",
    "\n",
    "DataAnalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02329e8e",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data as usual\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "3289db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "4f63f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "73eb5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Login page\n",
    "login_page= driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "login_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "cb18bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Search bar for Job , location and enter data\n",
    "search_bar= driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_bar.send_keys('Sunglasses')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "d077b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "2eb670bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "dc22492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "product=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "for i in range(1,4):\n",
    "    url='https://www.flipkart.com/search?q=Sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page='+str(i)\n",
    "    driver.get(url)\n",
    "    brand_tags= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "    product_tags= driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]')\n",
    "    price_tags= driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"] ')\n",
    "    discount_tags= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "    \n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text)\n",
    "    for i in product_tags:\n",
    "        product.append(i.text)            \n",
    "    for i in price_tags:\n",
    "        price.append(i.text.replace('₹',''))\n",
    "    for i in discount_tags:\n",
    "        discount.append(i.text[0:2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "e0a06786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Product details from parent class\n",
    "Product=[]\n",
    "for i in product:\n",
    "    Product.append(i.split('\\n')[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "ce40ed68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 120, 120, 120)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand), len(product),len(price),len(discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "84191502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>148</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized Wayfarer Sunglasses (...</td>\n",
       "      <td>193</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>208</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>179</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>629</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (58)</td>\n",
       "      <td>329</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (58)</td>\n",
       "      <td>424</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Mirrored Aviator Sunglasses (32)</td>\n",
       "      <td>149</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>1,119</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>113</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand                                            Product  Price  \\\n",
       "1         SRPM             UV Protection Wayfarer Sunglasses (50)    148   \n",
       "2       SUNBEE  UV Protection, Polarized Wayfarer Sunglasses (...    193   \n",
       "3    Elligator                UV Protection Round Sunglasses (54)    208   \n",
       "4       PIRASO              UV Protection Aviator Sunglasses (54)    179   \n",
       "5     Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...    629   \n",
       "..         ...                                                ...    ...   \n",
       "96   ROYAL SON                   Mirrored Aviator Sunglasses (58)    329   \n",
       "97   ROYAL SON         UV Protection Retro Square Sunglasses (58)    424   \n",
       "98      PIRASO                   Mirrored Aviator Sunglasses (32)    149   \n",
       "99    Fastrack              UV Protection Aviator Sunglasses (58)  1,119   \n",
       "100  New Specs   UV Protection Rectangular Sunglasses (Free Size)    113   \n",
       "\n",
       "    Discount  \n",
       "1         88  \n",
       "2         85  \n",
       "3         91  \n",
       "4         88  \n",
       "5         21  \n",
       "..       ...  \n",
       "96        78  \n",
       "97        71  \n",
       "98        90  \n",
       "99        13  \n",
       "100       92  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sunglass=pd.DataFrame({'Brand':brand[0:100],'Product':Product[0:100],'Price':price[0:100],'Discount':discount[0:100]})\n",
    "Sunglass.index +=1\n",
    "\n",
    "Sunglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882ac35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e128110",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3f5ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29a5ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73dd157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \" All 5902 reviews\"\n",
    "\n",
    "all_review=driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/a/div/span')\n",
    "all_review.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca0c8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brilliant</td>\n",
       "      <td>5</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>5</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>5</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>5</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Just wow!</td>\n",
       "      <td>5</td>\n",
       "      <td>Not mere a phone , Its more than that for fun ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice 👌👌👌👌👌👌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>5</td>\n",
       "      <td>MD sufiyan rider Owsm\\nMobile nd battery mast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Wonderful</td>\n",
       "      <td>5</td>\n",
       "      <td>Ok, so after almost 3 years I am again back in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Nice product</td>\n",
       "      <td>4</td>\n",
       "      <td>If you are looking for a premium phone under 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Review Rating  \\\n",
       "1              Brilliant      5   \n",
       "2         Simply awesome      5   \n",
       "3    Best in the market!      5   \n",
       "4       Perfect product!      5   \n",
       "5              Fabulous!      5   \n",
       "..                   ...    ...   \n",
       "96             Just wow!      5   \n",
       "97      Perfect product!      5   \n",
       "98             Fabulous!      5   \n",
       "99             Wonderful      5   \n",
       "100         Nice product      4   \n",
       "\n",
       "                                           Full Review  \n",
       "1    The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "2    Really satisfied with the Product I received.....  \n",
       "3    Great iPhone very snappy experience as apple k...  \n",
       "4    Amazing phone with great cameras and better ba...  \n",
       "5    This is my first iOS phone. I am very happy wi...  \n",
       "..                                                 ...  \n",
       "96   Not mere a phone , Its more than that for fun ...  \n",
       "97                                         Nice 👌👌👌👌👌👌  \n",
       "98   MD sufiyan rider Owsm\\nMobile nd battery mast ...  \n",
       "99   Ok, so after almost 3 years I am again back in...  \n",
       "100  If you are looking for a premium phone under 5...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review=[]\n",
    "rating=[]\n",
    "fullreview=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,11):\n",
    "    review_tags= driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "    rating_tags= driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\" or @class=\"_3LWZlK _1rdVr6 _1BLPMq\"]')\n",
    "    fullreview_tags=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]  ')\n",
    "    for i in review_tags:\n",
    "        review.append(i.text)\n",
    "    for i in rating_tags:\n",
    "        rating.append(i.text)\n",
    "    for i in fullreview_tags:\n",
    "        fullreview.append(i.text)\n",
    "\n",
    "    next_button =driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')\n",
    "    \n",
    "    try:\n",
    "        driver.get(next_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(next_button[0].get_attribute('href'))\n",
    "    \n",
    "    \n",
    "\n",
    "Iphone=pd.DataFrame({'Review':review[0:100],'Rating':rating[0:100],'Full Review':fullreview[0:100]})\n",
    "Iphone.index +=1\n",
    "Iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ba6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be1aee98",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13715125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c17e6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ffbd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Login Page\n",
    "login_page= driver.find_element_by_xpath('/html/body/div[2]/div/div/button')\n",
    "login_page.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "329c1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar= driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input ')\n",
    "search_bar.send_keys('sneakers')\n",
    "search_button= driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button ')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77c06ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>630</td>\n",
       "      <td>36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HRX by Hrithik Roshan</td>\n",
       "      <td>Men White Printed Sneakers Sneakers For Men</td>\n",
       "      <td>2,049</td>\n",
       "      <td>46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>469</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>327</td>\n",
       "      <td>67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>209</td>\n",
       "      <td>83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>590</td>\n",
       "      <td>40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>asics</td>\n",
       "      <td>LYTECOURT Sneakers For Men</td>\n",
       "      <td>2,001</td>\n",
       "      <td>55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>RODDICK SHOES</td>\n",
       "      <td>Fashion Outdoor Canvas Casual Light Weight Lac...</td>\n",
       "      <td>399</td>\n",
       "      <td>60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>SPARX</td>\n",
       "      <td>SM-439 Sneakers For Men</td>\n",
       "      <td>799</td>\n",
       "      <td>5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>590</td>\n",
       "      <td>40%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Brand                                            Product  \\\n",
       "1                 RapidBox                                   Sneakers For Men   \n",
       "2    HRX by Hrithik Roshan        Men White Printed Sneakers Sneakers For Men   \n",
       "3                   Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...   \n",
       "4                 Magnolia                                   Sneakers For Men   \n",
       "5                   BRUTON      Modern Trendy Sneakers Shoes Sneakers For Men   \n",
       "..                     ...                                                ...   \n",
       "96                RapidBox                                   Sneakers For Men   \n",
       "97                   asics                         LYTECOURT Sneakers For Men   \n",
       "98           RODDICK SHOES  Fashion Outdoor Canvas Casual Light Weight Lac...   \n",
       "99                   SPARX                            SM-439 Sneakers For Men   \n",
       "100               RapidBox                                   Sneakers For Men   \n",
       "\n",
       "     Price Discount  \n",
       "1      630      36%  \n",
       "2    2,049      46%  \n",
       "3      469      70%  \n",
       "4      327      67%  \n",
       "5      209      83%  \n",
       "..     ...      ...  \n",
       "96     590      40%  \n",
       "97   2,001      55%  \n",
       "98     399      60%  \n",
       "99     799       5%  \n",
       "100    590      40%  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand=[]\n",
    "product=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,4):\n",
    "    brand_tags= driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "    product_tags= driver.find_elements_by_xpath('//div[@class=\"_2B099V\"]')\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]  ')\n",
    "    discount_tags= driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "    \n",
    "    \n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text)\n",
    "    for i in product_tags:\n",
    "        product.append(i.text.split('\\n')[1])\n",
    "    for i in price_tags:\n",
    "        price.append(i.text.replace('₹',''))\n",
    "        \n",
    "    for i in discount_tags:\n",
    "        discount.append(i.text.split(' ')[0])\n",
    "\n",
    "    next_button =driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')\n",
    "    \n",
    "    try:\n",
    "        driver.get(next_button[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(next_button[0].get_attribute('href'))\n",
    "    \n",
    "    \n",
    "\n",
    "Iphone=pd.DataFrame({'Brand':brand[0:100],'Product':product[0:100],'Price':price[0:100],'Discount':discount[0:100]})\n",
    "Iphone.index +=1\n",
    "Iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755881c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f35edd",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image.\n",
    "\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "6614a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "ca190194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "57ec6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Filter  Colour checkbox:\n",
    "\n",
    "\n",
    "Colour_Filter= driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "Colour_Filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "a93e843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Filter Price checkbox\n",
    "Price_Filter= driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "Price_Filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "4ff5a250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sole To Soul</td>\n",
       "      <td>High-Top Block Heeled Boots</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROSSO BRUNELLO</td>\n",
       "      <td>Printed PU Kitten Sandals</td>\n",
       "      <td>8499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sole To Soul</td>\n",
       "      <td>Suede High-Top Block Heeled Boots</td>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sole To Soul</td>\n",
       "      <td>High-Top Platform Heeled Boots</td>\n",
       "      <td>8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ROSSO BRUNELLO</td>\n",
       "      <td>PU Block Pumps with Buckles</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Leather Block Heeled Boots with Bows</td>\n",
       "      <td>11815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>DAVINCHI</td>\n",
       "      <td>Men Textured Formal Leather Loafers</td>\n",
       "      <td>8990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Suede Party High-Top Block Heeled Boots</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>DAVINCHI</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Leather High-Top Heeled Boots</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                              Description  Price\n",
       "1      Sole To Soul              High-Top Block Heeled Boots   7900\n",
       "2    ROSSO BRUNELLO                Printed PU Kitten Sandals   8499\n",
       "3      Sole To Soul        Suede High-Top Block Heeled Boots   9500\n",
       "4      Sole To Soul           High-Top Platform Heeled Boots   8900\n",
       "5    ROSSO BRUNELLO              PU Block Pumps with Buckles   7999\n",
       "..              ...                                      ...    ...\n",
       "146         Saint G     Leather Block Heeled Boots with Bows  11815\n",
       "147        DAVINCHI      Men Textured Formal Leather Loafers   8990\n",
       "148         Saint G  Suede Party High-Top Block Heeled Boots   8925\n",
       "149        DAVINCHI          Men Solid Leather Formal Derbys   9990\n",
       "150         Saint G            Leather High-Top Heeled Boots  10625\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand=[]\n",
    "product=[]\n",
    "price=[]\n",
    "\n",
    "url=[]\n",
    "nex= driver.find_elements_by_xpath('//ul[@class=\"pagination-container\"]/li/a')\n",
    "for i in nex:\n",
    "    #print(i.get_attribute('href'))\n",
    "    url.append(i.get_attribute('href'))\n",
    "\n",
    "\n",
    "for i in url[0:3]:\n",
    "    driver.get(i)\n",
    "    brand_tags= driver.find_elements_by_xpath('//div[@class=\"product-productMetaInfo\"]/h3')\n",
    "    product_tags= driver.find_elements_by_xpath('//div[@class=\"product-productMetaInfo\"]/h4')\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"product-price\"]  ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in brand_tags:\n",
    "        brand.append(i.text)\n",
    "    for i in product_tags:\n",
    "        product.append(i.text)\n",
    "    for i in price_tags:\n",
    "        price.append(i.text.split(' ')[1].replace('Rs.',''))\n",
    "        \n",
    "        \n",
    "# SHowing all 150 products from 3 pages, can use [1:101] to show only  \n",
    "data= pd.DataFrame({'Brand':brand,'Description':product[::2],'Price':price})\n",
    "data.index+=1\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb619df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078ad39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c3a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b30b7b30",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "    \n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "16cd6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "7d8ea907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "55a043fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search bar, enter laptop\n",
    "search_bar= driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_bar.send_keys('Laptop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "d3243b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the keyword Laptop\n",
    "search_click= driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "f09d15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Filter\n",
    "i7= driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[11]/span/a/div/label/i')\n",
    "i7.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "f6491b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>96,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG Gram 16 inches Intel Evo 11th Gen Core i7 U...</td>\n",
       "      <td>89,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>77,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>96,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LG Gram 14 Intel Evo 11th Gen Core i7 14 inche...</td>\n",
       "      <td>88,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...</td>\n",
       "      <td>85,790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo ThinkBook Yoga 14s Intel Core i7 11th G...</td>\n",
       "      <td>87,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title   Price\n",
       "1   LG Gram Intel Evo 11th Gen Core i7 17 inches U...  96,999\n",
       "2   LG Gram 16 inches Intel Evo 11th Gen Core i7 U...  89,999\n",
       "3   Mi Notebook Ultra 3.2K Resolution Display Inte...  77,499\n",
       "4   ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  57,490\n",
       "5   ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...  89,990\n",
       "6   LG Gram Intel Evo 11th Gen Core i7 17 inches U...  96,999\n",
       "7   LG Gram 14 Intel Evo 11th Gen Core i7 14 inche...  88,990\n",
       "8   Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...  85,790\n",
       "9   Lenovo ThinkBook Yoga 14s Intel Core i7 11th G...  87,990\n",
       "10  Lenovo ThinkBook 13s Intel 11th Gen Core i7 13...  89,990"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title=[]\n",
    "rating=[]\n",
    "price=[]\n",
    "\n",
    "title_tags= driver.find_elements_by_xpath('//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "rating_tags= driver.find_elements_by_xpath('//div[@class=\"product-productMetaInfo\"]/h4')\n",
    "price_tags=driver.find_elements_by_xpath('//div[@class=\"a-row a-size-base a-color-base\"] ')\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in title_tags:\n",
    "    title.append(i.text)\n",
    "for i in rating_tags:\n",
    "    rating.append(i.text)\n",
    "for i in price_tags:\n",
    "    price.append(i.text.split('\\n')[0].replace(\"₹\",''))\n",
    "    \n",
    "data= pd.DataFrame({'Title':title[0:10],'Price':price[0:10]})\n",
    "data.index +=1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7010e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "e7efdc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "4e9ec938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "6d602f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "75c9196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_tags= driver.find_elements_by_xpath('//div[@class=\"a-section a-spacing-small a-spacing-top-small\"]/div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "f6d6a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=[]\n",
    "for i in rating_tags:\n",
    "    rat.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "2d1a806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating Ratings\n",
    "\n",
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")#collecting urls of all the laptop\n",
    "\n",
    "UR=[]\n",
    "\n",
    "for i in urls[:10]:\n",
    "\n",
    "    UR.append(i.get_attribute('href'))#getting the url of first 10 laptops\n",
    "\n",
    "for url in UR:#loop for every laptop in the list\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    try:                  #exception handling for nosuchelementexception                                                    #click the rating link found\n",
    "\n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']//span\")#locating the rating\n",
    "\n",
    "        Ratings.append(rating.text)#appending the ratings in Ratings list\n",
    "\n",
    "        \n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        Ratings.append(\"NO rating\")#appending the No rating if no rating is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68441930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "a0995a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "8fb99f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "UR=[]\n",
    "for i in urls[:10]:\n",
    "    UR.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "c53354e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "e646fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "for url in UR:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rating=driver.find_element_by_id(\"acrPopover\")\n",
    "        ratings.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        ratings.append('No rating')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "57d05c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "59a501c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "977e1273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=driver.find_elements_by_xpath(\"//span[@class='reviewCountTextLinkedHistogram noUnderline']\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "21d89615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_elements_by_id(\"//div[@id='acrPopover']/span/a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf48d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c35e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c3157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "727a4a15",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter “Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "48ac05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "248adf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "1539d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLick on Jobs option\n",
    "\n",
    "job= driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[6]')\n",
    "job.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "bcd6b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Bar\n",
    "search_bar= driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/div/span/input')\n",
    "search_bar.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "9b90a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "search= driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/button/span')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f6015276",
   "metadata": {},
   "outputs": [],
   "source": [
    "location= driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[2]/div[1]/div/div/div/div[2]/div[1]/p')\n",
    "location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9c1452c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_search= driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[5]/div/label')\n",
    "location_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "75f9a8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of Days Ago</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMITED</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HCL Technologies Limited</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>16d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WSP CONSULTANTS INDIA PRIVATE LIMITED</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Microsoft India (R and D) Pvt Ltd</td>\n",
       "      <td>17d ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HCL Technologies Ltd</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jubilant Foodworks Limited</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>21d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hanu Software Solutions Pvt Ltd</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tavant Technologies India Pvt. Ltd.</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Company Name No. of Days Ago Ratings\n",
       "1   NTT DATA GLOBAL DELIVERY SERVICES PRIVATE LIMITED          5d ago     3.9\n",
       "2                            HCL Technologies Limited          6d ago     3.8\n",
       "3      Optum Global Solutions (India) Private Limited         16d ago     4.1\n",
       "4               WSP CONSULTANTS INDIA PRIVATE LIMITED          6d ago     4.2\n",
       "5                   Microsoft India (R and D) Pvt Ltd         17d ago     4.2\n",
       "6                                HCL Technologies Ltd         14d ago     3.8\n",
       "7                          Jubilant Foodworks Limited          4d ago     3.9\n",
       "8                                    HCL Technologies         21d ago     3.8\n",
       "9                     Hanu Software Solutions Pvt Ltd          6d ago     3.7\n",
       "10                Tavant Technologies India Pvt. Ltd.        1mon ago     4.0"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[]\n",
    "daysago=[]\n",
    "rating=[]\n",
    "\n",
    "name_tags= driver.find_elements_by_xpath('//p[@class=\"company body-medium\"]')\n",
    "daysago_tags= driver.find_elements_by_xpath('//*[contains(text(),\"ago\")]')\n",
    "rating_tags=driver.find_elements_by_xpath('//div[@class=\"company-info\"]/div' )\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in name_tags:\n",
    "    name.append(i.text)\n",
    "for i in daysago_tags:\n",
    "    daysago.append(i.text)\n",
    "for i in rating_tags:\n",
    "    rating.append(i.text.split('\\n')[0])\n",
    "    \n",
    "# No of days and Ratings have 1st elemnt duplicate so removing it\n",
    "data= pd.DataFrame({'Company Name':name,'No. of Days Ago': daysago[1:],'Ratings':rating[1:]})\n",
    "data.index+=1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d52418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a963a0cd",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and then click on “Data Scientist\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe.\n",
    "Note: All the steps required during scraping should be done through code only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "eb02c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraraies\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "8d3d03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "2246fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets connect to driver\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\Bank of America\\Downloads\\chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "url='https://www.ambitionbox.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "ae6450ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLick on Salary option\n",
    "\n",
    "job= driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[4]')\n",
    "job.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "19b0ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Column\n",
    "search= driver.find_element_by_xpath('/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input')\n",
    "search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "89c92004",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedTagNameException",
     "evalue": "Message: Select only works on <select> elements, not on <div>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedTagNameException\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BANKOF~1\\AppData\\Local\\Temp/ipykernel_8452/3353167297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclk\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mSelect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\select.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, webelement)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwebelement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"select\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             raise UnexpectedTagNameException(\n\u001b[0m\u001b[0;32m     38\u001b[0m                 \u001b[1;34m\"Select only works on <select> elements, not on <%s>\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 webelement.tag_name)\n",
      "\u001b[1;31mUnexpectedTagNameException\u001b[0m: Message: Select only works on <select> elements, not on <div>\n"
     ]
    }
   ],
   "source": [
    "clk= Select(driver.find_element_by_xpath('/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "6662f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"bd41b7ce43edea16de19eda213994c71\", element=\"b828af54-2db0-4596-aa8e-f2445c0dd4ae\")>"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8a765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
